{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "\n",
    "from models.experimental import attempt_load\n",
    "from utils.datasets import LoadStreams, LoadImages\n",
    "from utils.general import check_img_size, check_requirements, check_imshow, non_max_suppression, apply_classifier, \\\n",
    "    scale_coords, xyxy2xywh, strip_optimizer, set_logging, increment_path, save_one_box, \\\n",
    "    polygon_non_max_suppression, polygon_scale_coords\n",
    "from utils.plots import colors, plot_one_box, polygon_plot_one_box\n",
    "from utils.torch_utils import select_device, load_classifier, time_synchronized\n",
    "\n",
    "import subprocess as sp\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "# view_img=False,  # show results\n",
    "# save_txt=False,  # save results to *.txt\n",
    "# save_conf=False,  # save confidences in --save-txt labels\n",
    "# ,  # save cropped prediction boxes\n",
    "# nosave=False,  # do not save images/videos\n",
    "# ,  # augmented inference\n",
    "# update=False,  # update all models\n",
    "# project='runs/detect',  # save results to project/name\n",
    "# name='exp',  # save results to project/name\n",
    "# exist_ok=False,  # existing project/name ok, do not increment\n",
    "# line_thickness=3,  # bounding box thickness (pixels)\n",
    "# hide_labels=False,  # hide labels\n",
    "# hide_conf=False,  # hide confidences\n",
    "# half=False,  # use FP16 half-precision inference\n",
    "\n",
    "###################\n",
    "augment=False\n",
    "conf_thres=0.25\n",
    "iou_thres=0.25\n",
    "classes=None\n",
    "agnostic_nms=False\n",
    "max_det=1000 # maximum detections per image\n",
    "device='2'\n",
    "# weights='runs/train/exp52/weights/polygon_best.pt'\n",
    "weights='runs/train/exp55/weights/polygon_last.pt'\n",
    "imgsz=640\n",
    "# source='testsample/0002.png'\n",
    "###################\n",
    "\n",
    "device = select_device(device)\n",
    "\n",
    "# Load model\n",
    "model = attempt_load(weights, map_location=device)  # load FP32 model\n",
    "stride = int(model.stride.max())  # model stride\n",
    "imgsz = check_img_size(imgsz, s=stride)  # check image size\n",
    "\n",
    "print(stride)\n",
    "\n",
    "def detect(source):\n",
    "    # Set Dataloader\n",
    "    dataset = LoadImages(source, img_size=imgsz, stride=stride)\n",
    "\n",
    "    # Run inference\n",
    "    if device.type != 'cpu':\n",
    "        with torch.no_grad():\n",
    "            model(torch.zeros(1, 3, imgsz, imgsz).to(device).type_as(next(model.parameters())))  # run once\n",
    "\n",
    "    for path, img, im0s, vid_cap in dataset:\n",
    "        img = torch.from_numpy(img).to(device)\n",
    "        img = img.float()  # uint8 to fp16/32\n",
    "        img /= 255.0  # 0 - 255 to 0.0 - 1.0\n",
    "        if img.ndimension() == 3:\n",
    "            img = img.unsqueeze(0)\n",
    "\n",
    "        # Inference\n",
    "        pred = model(img, augment=augment)[0]\n",
    "\n",
    "        # Apply polygon NMS\n",
    "        pred = polygon_non_max_suppression(pred, conf_thres, iou_thres, classes, agnostic_nms, max_det=max_det)\n",
    "\n",
    "        results = []\n",
    "        # Process detections\n",
    "        for i, det in enumerate(pred):  # detections per image\n",
    "    #         if webcam:  # batch_size >= 1\n",
    "    #             p, s, im0, frame = path[i], f'{i}: ', im0s[i].copy(), dataset.count\n",
    "    #         else:\n",
    "            p, s, im0, frame = path, '', im0s.copy(), getattr(dataset, 'frame', 0)\n",
    "\n",
    "            gn = torch.tensor(im0.shape)[[1, 0, 1, 0, 1, 0, 1, 0]]  # normalization gain xyxyxyxy\n",
    "\n",
    "            lines = []\n",
    "            if len(det):\n",
    "                # Rescale boxes from img_size to im0 size\n",
    "                det[:, :8] = polygon_scale_coords(img.shape[2:], det[:, :8], im0.shape).round()\n",
    "                # Print results\n",
    "                \n",
    "                # Write results\n",
    "                for *xyxyxyxy, conf, cls in reversed(det):\n",
    "                    xyxyxyxyn = (torch.tensor(xyxyxyxy).view(1, 8)).view(-1).tolist()  # normalized xyxyxyxy\n",
    "                    line = (cls, *xyxyxyxyn, conf)  # label format\n",
    "                    lines.append(line[1:-1])\n",
    "#                     print(('%g ' * len(line)).rstrip() % line + '\\n')\n",
    "            results.append(lines)\n",
    "        return np.array(results, dtype=np.int32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(1, 1, 8)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detect('video_example2/0000.png').shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for path, img, im0s, vid_cap in dataset:\n",
    "    print(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import cv2\n",
    "import glob\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "show = False\n",
    "save = True\n",
    "# root = '/home/workspace/salim/data_set/20211123_1013_exposure_-10'\n",
    "roots = glob.glob('/home/workspace/salim/data_set/*')\n",
    "\n",
    "cnt = 0\n",
    "\n",
    "for root in roots:\n",
    "    \n",
    "    root = '/home/workspace/salim/data_set/yuginong_banana'\n",
    "    save_root = root.replace('data_set','data_set_crop_test')\n",
    "#     if '50' in root:\n",
    "#         continue\n",
    "#     if os.path.isdir(save_root):\n",
    "#         continue\n",
    "    \n",
    "    impath = glob.glob(root+'/*.png')\n",
    "    impath = sorted(impath)\n",
    "\n",
    "    # impath = glob.glob(\"runs/detect/exp65/*.png\")\n",
    "    cache = []\n",
    "    for path in impath:\n",
    "#         if cnt < 350:\n",
    "#             cnt += 1\n",
    "#             continue\n",
    "    #     string = f\"{path}\"\n",
    "    #     print(path)\n",
    "        img = cv2.imread(path)\n",
    "\n",
    "        anos = detect(path)[0]\n",
    "        centers = []\n",
    "        box_cnt = 0\n",
    "        for ano in anos:\n",
    "            ## edge에 가까운 것 제거\n",
    "            if any(ano < 10):\n",
    "                continue\n",
    "            if any(ano[::2] > img.shape[1]-10):\n",
    "                continue\n",
    "            if any(ano[1::2] > img.shape[0]-10):\n",
    "                continue\n",
    "\n",
    "            point1, point2, point3, point4 = ano[:2], ano[2:4], ano[4:6], ano[6:]\n",
    "\n",
    "            center = (point1+point2+point3+point4)/4\n",
    "            center = center.astype(np.int32)\n",
    "    #         string += f\" {list(center)}\"\n",
    "    #         print(center)\n",
    "            if show:\n",
    "                cv2.line(img, point1,point1,(255,0,0),25)\n",
    "                cv2.line(img, point2,point2,(255,127,0),25)\n",
    "                cv2.line(img, point3,point3,(255,255,0),25)\n",
    "                cv2.line(img, center,center,(0,255,255),25)\n",
    "\n",
    "            new_w = int(np.linalg.norm(point1-point2))\n",
    "            new_h = int(np.linalg.norm(point2-point3))\n",
    "\n",
    "            ## 비슷한 위치의 라벨이 2개 이상일 때, 이상데이터 제거\n",
    "            wh = float(new_w) + float(new_h)\n",
    "            if len(anos) >= 2: #라벨 2개 이상\n",
    "#                 print(sum(ano))\n",
    "                sum_ano = sum(ano)\n",
    "                duplicate = False\n",
    "                for ano_ in anos:\n",
    "                    if sum_ano == sum(ano_):\n",
    "                        continue\n",
    "                    if 0.95 * sum(ano_) < sum_ano and sum_ano < 1.05 * sum(ano_): # 비슷한 위치인가\n",
    "                        duplicate = True\n",
    "                        break\n",
    "                if duplicate:\n",
    "#                     print(sum(cache)/len(cache))\n",
    "#                     print(wh)\n",
    "                    if wh < 0.9 * sum(cache)/len(cache) or wh > 1.1 * sum(cache)/len(cache): # 이상 데이터면, 스킵(제거)\n",
    "                        continue\n",
    "                        \n",
    "            centers.append(list(center))\n",
    "            cache.append(wh)\n",
    "            if len(cache) > 5:\n",
    "                cache.pop(0)\n",
    "            \n",
    "            pts1 = np.float32([point1,point2,point3])\n",
    "            pts2 = np.float32([[0,0], [new_w, 0], [new_w, new_h]])\n",
    "            mat = cv2.getAffineTransform(pts1,pts2)\n",
    "            dst = cv2.warpAffine(img, mat, None)\n",
    "            dst = dst[:new_h,:new_w]\n",
    "\n",
    "            if show:\n",
    "                if len(anos) > 0:\n",
    "                    fig = plt.figure()\n",
    "                    fig.set_size_inches(10,10)\n",
    "                    plt.imshow(img)\n",
    "                    plt.show()\n",
    "                    plt.imshow(dst)\n",
    "                    plt.show()\n",
    "            if save:\n",
    "                os.makedirs(save_root, exist_ok=True)\n",
    "                cv2.imwrite(os.path.join(save_root, os.path.basename(path).replace('.png',f\"_{box_cnt}.png\")),dst)\n",
    "            box_cnt += 1\n",
    "\n",
    "        result_dict = {}\n",
    "        result_dict['path'] = path\n",
    "        result_dict['centers'] = centers\n",
    "        with open(save_root+'.txt',\"a\") as f:\n",
    "            f.writelines(str(result_dict)+'\\n')\n",
    "        print(result_dict)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.0541436464088396"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1335.6/1267"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache = []\n",
    "cache.append(1)\n",
    "cache.append(2)\n",
    "cache.append(3)\n",
    "cache.pop()\n",
    "cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/workspace/salim/data_set/20211123_1013_exposure_-10/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "root"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolopoly",
   "language": "python",
   "name": "yolopoly"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
